================================================================================
PLACE CONFLATION MODEL EVALUATION: COMPARATIVE ANALYSIS & RECOMMENDATION
================================================================================

üéØ OBJECTIVE: Evaluate improvement of place conflation using language models

üìä KEY RESULTS:
  1. Achieve at least 80% F1 score (or precision/recall balance) on the test dataset using a language model
  2. Run inference within 50 ms per match on average, using a low-cost model
  3. Identify and recommend the model with the best price-to-performance ratio among baseline and small LLM

üöÄ LOADING DATA...
‚úì Dataset: 3000 records
‚úì Ground truth: 44.4% match rate
‚úì Split: 2400 train, 600 test

============================================================
BASELINE COMPARISON
============================================================
üìä Previous Matcher: 44.4% accuracy, ~1ms, $0.00

================================================================================
LANGUAGE MODEL EVALUATION: MULTI-MODEL COMPARISON
================================================================================

ü§ñ Evaluating all-MiniLM-L6-v2...
‚úì Model loaded

RESULTS:
Precision: 0.8063
Recall: 0.8577
F1 Score: 0.8312
Time per match: 21.28ms
Threshold: 0.840

ü§ñ Evaluating paraphrase-MiniLM-L6-v2...
‚úì Model loaded

RESULTS:
Precision: 0.7801
Recall: 0.8240
F1 Score: 0.8015
Time per match: 24.16ms
Threshold: 0.840

ü§ñ Evaluating all-mpnet-base-v2...
‚úì Model loaded

RESULTS:
Precision: 0.7500
Recall: 0.8315
F1 Score: 0.7886
Time per match: 110.37ms
Threshold: 0.850
‚ö†Ô∏è  Model exceeds speed requirement (110.4ms > 50ms), skipping further evaluation

================================================================================
COMPARATIVE ANALYSIS REPORT: MODEL PERFORMANCE EVALUATION
================================================================================

üìä PERFORMANCE METRICS COMPARISON:

Model                               F1 Score     Precision    Recall       Speed (ms)   Cost/1M      Size (MB)   
----------------------------------- ------------ ------------ ------------ ------------ ------------ ------------
Previous Matcher (Baseline)         44.4%        N/A          N/A          1.0          $0.00        0           
all-MiniLM-L6-v2                    83.1%        0.806        0.858        21.3         $0.10        22          
paraphrase-MiniLM-L6-v2             80.1%        0.780        0.824        24.2         $0.10        22          
all-mpnet-base-v2                   78.9%        0.750        0.831        110.4        $0.10        420         

================================================================================
SUMMARY & RECOMMENDATION
================================================================================

üèÜ Top Performer (F1 Score): all-MiniLM-L6-v2 - 83.1% F1
üí∞ Top Price-Performer: all-MiniLM-L6-v2 - Score: 39057.92

OKR EVALUATION TABLE:
Model                               KR1 (F1‚â•80%)    KR2 (‚â§50ms)     KR3 (Best P/P) 
----------------------------------- --------------- --------------- ---------------
Previous Matcher (Baseline)         N/A             ‚úÖ               N/A            
all-MiniLM-L6-v2                    ‚úÖ               ‚úÖ               ‚úÖ              
paraphrase-MiniLM-L6-v2             ‚úÖ               ‚úÖ               ‚ùå              
all-mpnet-base-v2                   ‚ùå 78.9%         ‚ùå 110.4ms       ‚ùå              

================================================================================
FINAL RECOMMENDATION
================================================================================

‚úÖ RECOMMENDED MODEL: all-MiniLM-L6-v2

Rationale:
  ‚Ä¢ F1 Score: 83.1% (+38.7% vs baseline)
  ‚Ä¢ Speed: 21.3ms per match
  ‚Ä¢ Price-Performance: Composite Score 39057.92
  ‚Ä¢ OKRs Met: KR1 ‚úÖ, KR2 ‚úÖ, KR3 ‚úÖ

================================================================================
